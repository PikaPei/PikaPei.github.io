<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>information theory on PikaPei&#39;s Website</title>
    <link>https://PikaPei.github.io/categories/information-theory/</link>
    <description>Recent content in information theory on PikaPei&#39;s Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 27 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://PikaPei.github.io/categories/information-theory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>神經科學與資訊理論 - Part 2</title>
      <link>https://PikaPei.github.io/blog/20210927-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part2/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://PikaPei.github.io/blog/20210927-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part2/</guid>
      <description>神經科學與資訊理論 - Part 1
前文中提及了不確定性與資訊熵的概念，但仍侷限在只有一個變數的情況，這篇文章將從一個變數增加至兩個變數，介紹聯合熵、條件熵，最後引入相互資訊和不確定性的關係。 聯合熵 (Joint Entropy) 若是系統含有多於一個以上的變數，使用聯合熵：
$$H(X,Y) = - \sum_{x \in X,y \in Y} P(x,y)\ \log_{2} P(x,y)$$
  丟擲一枚硬幣，並且從黑桃、紅心、方塊、梅花四張A中任抽一張，會有以下八種組合，機率各1/8。
\(X=\{正面,反面\}\)
\(Y=\{黑桃,紅心,方塊,梅花\}\)
    黑桃 紅心 方塊 梅花     正面 1/8 1/8 1/8 1/8   反面 1/8 1/8 1/8 1/8    $$ \begin{aligned} H(X,Y) &amp;amp;= - \sum_{\substack{x \in {heads,tails} \\ y \in {spades,hearts,diamonds,clubs}}} P(x,y)\ \log_{2} P(x,y) \\ &amp;amp;= 8\cdot-[\frac{1}{8} \log_2(\frac{1}{8}) ] = 3 \end{aligned} $$</description>
    </item>
    
    <item>
      <title>神經科學與資訊理論 - Part 1</title>
      <link>https://PikaPei.github.io/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://PikaPei.github.io/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/</guid>
      <description>筆者亦初識資訊理論(information theory)，若理解有誤也請不吝指教。本文內容大多源於〈A tutorial for information theory in neuroscience〉這篇回顧論文，此篇以較淺白方式介紹資訊理論及其於神經科學上之應用。若有興趣深入的讀者，不妨參閱經典教科書《Elements of Information Theory》[1]、交通大學陳伯寧老師開設之線上課程[2]、或其他參考資料[3]。 為何需要資訊理論？ 我們常常會聽到，神經系統負責整合感官接收到的訊息，經過中樞處理後，再送往下游產生相應動作；又或者是，神經元相接形成神經網路，訊息在神經元間傳遞，大腦得以對其進行編碼、運算、儲存等等。&amp;ldquo;訊息&amp;rdquo; 等類似辭彙雖頻繁出現，但其含義也十分模糊，我們如何描述神經元攜帶了多少訊息？兩神經元是否共享了某些訊息？此類問題顯然需要一個量化訊息的方式，而資訊理論正是合適之工具。 資訊理論的優點  無需依賴對應模型(model independent)：不需先假設目標結構(如：事先假設好神經群之間的連結關係，才進行分析)，具有更廣泛的應用場景。 可同時分析不同型態的資料，包含離散型和連續型資料，也有助於分析跨尺度的交互關係(如神經元層級 vs 腦區層級)：  離散型資料：如有無產生動作電位、實驗動物有無產生特定行為模式 連續型資料：如膜電位變化、螢光強度變化、實驗動物位置或速度   可偵測線性和非線性的交互關係 可用於多變量分析 一般而言，輸出的單位皆為bits (後文會加以介紹)，因此在不同實驗結果下比較會相對直觀(但並非可以直接比較)。   資訊理論的限制  無法建構描述系統如何運作的模型：
如：分析結果得知，A與B共享了0.05 bits的資訊，但是我們無法更進一步知道A與B是否存在直接連結，即使有，也不知道是興奮性或抑制性連結。  話雖如此，仍能透過資訊理論排除不可能的模型，限縮尋找目標的範圍。 資訊熵 (Shannon Entropy) 首先，要得知訊息量多寡的方式，是這條訊息可以消除多少來自問題的不確定性(uncertainty)。
例如這個問題：今天晚餐要吃什麼？
A：都可以
B：校外好遠，在校內吃就好
C：吃小7
很明顯的，訊息量C &amp;gt; B &amp;gt; A，又A的回答不具任何訊息量。
因此，在測定訊息量之前，得先衡量出不確定性。Claude Shannon提出以資訊熵\(H(X)\) (Shannon entropy)來量化不確定性，不確定性越高，熵也就越大。
(註：熵一詞源自於熱力學，用以描述系統無序的程度，資訊熵和熱力學的熵二者在定義及概念下皆具有相似性。)
(註：為何是以\(H\)作為代號，可參考資料[4]。)
$$H(X) = - \sum_{x \in X} P(x)\ \log_{2}P(x)$$
公式中的\(X\)包含了所有可能狀態的\(x\)。   擲一公平硬幣，正面、反面機率皆為\(\frac{1}{2}\)，對於 &amp;ldquo;朝上的是哪一面？&amp;rdquo; 這個問題的不確定性：</description>
    </item>
    
  </channel>
</rss>
