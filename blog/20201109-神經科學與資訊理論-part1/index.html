<!doctype html>
<html lang="en">
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<head>
  <meta charset="utf-8">
<title>神經科學與資訊理論 - Part 1 - PikaPei&#39;s Website</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="generator" content="Hugo 0.81.0" /><meta itemprop="name" content="神經科學與資訊理論 - Part 1">
<meta itemprop="description" content="筆者亦初識資訊理論(information theory)，若理解有誤也請不吝指教。本文內容大多源於〈A tutorial for information theory in neuroscience〉這篇回顧論文，此篇以較淺白方式介紹資訊理論及其於神經科學上之應用。若有興趣深入的讀者，不妨參閱經典教科書《Elements of Information Theory》[1]、交通大學陳伯寧老師開設之線上課程[2]、或其他參考資料[3]。 為何需要資訊理論？ 我們常常會聽到，神經系統負責整合感官接收到的訊息，經過中樞處理後，再送往下游產生相應動作；又或者是，神經元相接形成神經網路，訊息在神經元間傳遞，大腦得以對其進行編碼、運算、儲存等等。&ldquo;訊息&rdquo; 等類似辭彙雖頻繁出現，但其含義也十分模糊，我們如何描述神經元攜帶了多少訊息？兩神經元是否共享了某些訊息？此類問題顯然需要一個量化訊息的方式，而資訊理論正是合適之工具。 資訊理論的優點  無需依賴對應模型(model independent)：不需先假設目標結構(如：事先假設好神經群之間的連結關係，才進行分析)，具有更廣泛的應用場景。 可同時分析不同型態的資料，包含離散型和連續型資料，也有助於分析跨尺度的交互關係(如神經元層級 vs 腦區層級)：  離散型資料：如有無產生動作電位、實驗動物有無產生特定行為模式 連續型資料：如膜電位變化、螢光強度變化、實驗動物位置或速度   可偵測線性和非線性的交互關係 可用於多變量分析 一般而言，輸出的單位皆為bits (後文會加以介紹)，因此在不同實驗結果下比較會相對直觀(但並非可以直接比較)。   資訊理論的限制  無法建構描述系統如何運作的模型：
如：分析結果得知，A與B共享了0.05 bits的資訊，但是我們無法更進一步知道A與B是否存在直接連結，即使有，也不知道是興奮性或抑制性連結。  話雖如此，仍能透過資訊理論排除不可能的模型，限縮尋找目標的範圍。 資訊熵 (Shannon Entropy) 首先，要得知訊息量多寡的方式，是這條訊息可以消除多少來自問題的不確定性(uncertainty)。
例如這個問題：今天晚餐要吃什麼？
A：都可以
B：校外好遠，在校內吃就好
C：吃小7
很明顯的，訊息量C &gt; B &gt; A，又A的回答不具任何訊息量。
因此，在測定訊息量之前，得先衡量出不確定性。Claude Shannon提出以資訊熵\(H(X)\) (Shannon entropy)來量化不確定性，不確定性越高，熵也就越大。
(註：熵一詞源自於熱力學，用以描述系統無序的程度，資訊熵和熱力學的熵二者在定義及概念下皆具有相似性。)
(註：為何是以\(H\)作為代號，可參考資料[4]。)
$$H(X) = - \sum_{x \in X} P(x)\ \log_{2}P(x)$$
公式中的\(X\)包含了所有可能狀態的\(x\)。   擲一公平硬幣，正面、反面機率皆為\(\frac{1}{2}\)，對於 &ldquo;朝上的是哪一面？&rdquo; 這個問題的不確定性："><meta itemprop="datePublished" content="2020-11-09T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-11-09T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="215"><meta itemprop="image" content="">
<meta itemprop="keywords" content="information theory," /><meta property="og:title" content="神經科學與資訊理論 - Part 1" />
<meta property="og:description" content="筆者亦初識資訊理論(information theory)，若理解有誤也請不吝指教。本文內容大多源於〈A tutorial for information theory in neuroscience〉這篇回顧論文，此篇以較淺白方式介紹資訊理論及其於神經科學上之應用。若有興趣深入的讀者，不妨參閱經典教科書《Elements of Information Theory》[1]、交通大學陳伯寧老師開設之線上課程[2]、或其他參考資料[3]。 為何需要資訊理論？ 我們常常會聽到，神經系統負責整合感官接收到的訊息，經過中樞處理後，再送往下游產生相應動作；又或者是，神經元相接形成神經網路，訊息在神經元間傳遞，大腦得以對其進行編碼、運算、儲存等等。&ldquo;訊息&rdquo; 等類似辭彙雖頻繁出現，但其含義也十分模糊，我們如何描述神經元攜帶了多少訊息？兩神經元是否共享了某些訊息？此類問題顯然需要一個量化訊息的方式，而資訊理論正是合適之工具。 資訊理論的優點  無需依賴對應模型(model independent)：不需先假設目標結構(如：事先假設好神經群之間的連結關係，才進行分析)，具有更廣泛的應用場景。 可同時分析不同型態的資料，包含離散型和連續型資料，也有助於分析跨尺度的交互關係(如神經元層級 vs 腦區層級)：  離散型資料：如有無產生動作電位、實驗動物有無產生特定行為模式 連續型資料：如膜電位變化、螢光強度變化、實驗動物位置或速度   可偵測線性和非線性的交互關係 可用於多變量分析 一般而言，輸出的單位皆為bits (後文會加以介紹)，因此在不同實驗結果下比較會相對直觀(但並非可以直接比較)。   資訊理論的限制  無法建構描述系統如何運作的模型：
如：分析結果得知，A與B共享了0.05 bits的資訊，但是我們無法更進一步知道A與B是否存在直接連結，即使有，也不知道是興奮性或抑制性連結。  話雖如此，仍能透過資訊理論排除不可能的模型，限縮尋找目標的範圍。 資訊熵 (Shannon Entropy) 首先，要得知訊息量多寡的方式，是這條訊息可以消除多少來自問題的不確定性(uncertainty)。
例如這個問題：今天晚餐要吃什麼？
A：都可以
B：校外好遠，在校內吃就好
C：吃小7
很明顯的，訊息量C &gt; B &gt; A，又A的回答不具任何訊息量。
因此，在測定訊息量之前，得先衡量出不確定性。Claude Shannon提出以資訊熵\(H(X)\) (Shannon entropy)來量化不確定性，不確定性越高，熵也就越大。
(註：熵一詞源自於熱力學，用以描述系統無序的程度，資訊熵和熱力學的熵二者在定義及概念下皆具有相似性。)
(註：為何是以\(H\)作為代號，可參考資料[4]。)
$$H(X) = - \sum_{x \in X} P(x)\ \log_{2}P(x)$$
公式中的\(X\)包含了所有可能狀態的\(x\)。   擲一公平硬幣，正面、反面機率皆為\(\frac{1}{2}\)，對於 &ldquo;朝上的是哪一面？&rdquo; 這個問題的不確定性：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://PikaPei.github.io/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/" /><meta property="og:image" content="" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2020-11-09T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-11-09T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="神經科學與資訊理論 - Part 1"/>
<meta name="twitter:description" content="筆者亦初識資訊理論(information theory)，若理解有誤也請不吝指教。本文內容大多源於〈A tutorial for information theory in neuroscience〉這篇回顧論文，此篇以較淺白方式介紹資訊理論及其於神經科學上之應用。若有興趣深入的讀者，不妨參閱經典教科書《Elements of Information Theory》[1]、交通大學陳伯寧老師開設之線上課程[2]、或其他參考資料[3]。 為何需要資訊理論？ 我們常常會聽到，神經系統負責整合感官接收到的訊息，經過中樞處理後，再送往下游產生相應動作；又或者是，神經元相接形成神經網路，訊息在神經元間傳遞，大腦得以對其進行編碼、運算、儲存等等。&ldquo;訊息&rdquo; 等類似辭彙雖頻繁出現，但其含義也十分模糊，我們如何描述神經元攜帶了多少訊息？兩神經元是否共享了某些訊息？此類問題顯然需要一個量化訊息的方式，而資訊理論正是合適之工具。 資訊理論的優點  無需依賴對應模型(model independent)：不需先假設目標結構(如：事先假設好神經群之間的連結關係，才進行分析)，具有更廣泛的應用場景。 可同時分析不同型態的資料，包含離散型和連續型資料，也有助於分析跨尺度的交互關係(如神經元層級 vs 腦區層級)：  離散型資料：如有無產生動作電位、實驗動物有無產生特定行為模式 連續型資料：如膜電位變化、螢光強度變化、實驗動物位置或速度   可偵測線性和非線性的交互關係 可用於多變量分析 一般而言，輸出的單位皆為bits (後文會加以介紹)，因此在不同實驗結果下比較會相對直觀(但並非可以直接比較)。   資訊理論的限制  無法建構描述系統如何運作的模型：
如：分析結果得知，A與B共享了0.05 bits的資訊，但是我們無法更進一步知道A與B是否存在直接連結，即使有，也不知道是興奮性或抑制性連結。  話雖如此，仍能透過資訊理論排除不可能的模型，限縮尋找目標的範圍。 資訊熵 (Shannon Entropy) 首先，要得知訊息量多寡的方式，是這條訊息可以消除多少來自問題的不確定性(uncertainty)。
例如這個問題：今天晚餐要吃什麼？
A：都可以
B：校外好遠，在校內吃就好
C：吃小7
很明顯的，訊息量C &gt; B &gt; A，又A的回答不具任何訊息量。
因此，在測定訊息量之前，得先衡量出不確定性。Claude Shannon提出以資訊熵\(H(X)\) (Shannon entropy)來量化不確定性，不確定性越高，熵也就越大。
(註：熵一詞源自於熱力學，用以描述系統無序的程度，資訊熵和熱力學的熵二者在定義及概念下皆具有相似性。)
(註：為何是以\(H\)作為代號，可參考資料[4]。)
$$H(X) = - \sum_{x \in X} P(x)\ \log_{2}P(x)$$
公式中的\(X\)包含了所有可能狀態的\(x\)。   擲一公平硬幣，正面、反面機率皆為\(\frac{1}{2}\)，對於 &ldquo;朝上的是哪一面？&rdquo; 這個問題的不確定性："/>
<link rel="stylesheet" href="/css/bundle.min.7af44880fa8f2867fbbf1b1e6758774b791bf321699b23f84be69e26e842c64a.css" integrity="sha256-evRIgPqPKGf7vxseZ1h3S3kb8yFpmyP4S&#43;aeJuhCxko="><link rel="stylesheet" href="/css/add-on.css">
</head>

  <body>
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          PikaPei
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/about/" class="nav link"><i class='far fa-id-card'></i> About</a>
        
      
      
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    
    
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="https://PikaPei.github.io/img/main/pika_logo.jpg" class="circle" width="100" alt="Hugo Future Imperfect Slim" /></a>
  <header>
    <h1>PikaPei</h1>
  </header>
  <main>
    <p>Research Assistant in <a href='http://life.nthu.edu.tw/~lablcc/index_ch.html' target='_blank'>CCLo's Lab</a></p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        
        <li><a href="//github.com/PikaPei" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>









































      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/">神經科學與資訊理論 - Part 1</a></h2>
    
    
  </div>
  <div class="meta">
    <time datetime="2020-11-09 00:00:00 &#43;0000 UTC">November 9, 2020</time>
    <p>Pei</p>
    <p>2-Minute Read</p>
  </div>
</header>

    <div id="socnet-share">
      




  
  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  


    </div>
    <div class="content">
      <a href="/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/" class="image" style="--bg-image: url('https://PikaPei.github.io/img/2020/1109/entropy.jpg');">
    <img src="https://PikaPei.github.io/img/2020/1109/entropy.jpg" alt="Proposed Model">
  </a>
      <p>筆者亦初識資訊理論(information theory)，若理解有誤也請不吝指教。本文內容大多源於〈A tutorial for information theory in neuroscience〉這篇回顧論文，此篇以較淺白方式介紹資訊理論及其於神經科學上之應用。若有興趣深入的讀者，不妨參閱經典教科書《Elements of Information Theory》[1]、交通大學陳伯寧老師開設之線上課程[2]、或其他參考資料[3]。
<br><br></p>
<h2 id="為何需要資訊理論">為何需要資訊理論？</h2>
<p>我們常常會聽到，神經系統負責整合感官接收到的訊息，經過中樞處理後，再送往下游產生相應動作；又或者是，神經元相接形成神經網路，訊息在神經元間傳遞，大腦得以對其進行編碼、運算、儲存等等。&ldquo;訊息&rdquo; 等類似辭彙雖頻繁出現，但其含義也十分模糊，我們如何描述神經元攜帶了多少訊息？兩神經元是否共享了某些訊息？此類問題顯然需要一個量化訊息的方式，而資訊理論正是合適之工具。
<br><br></p>
<h2 id="資訊理論的優點">資訊理論的優點</h2>
<ul>
<li>無需依賴對應模型(model independent)：不需先假設目標結構(如：事先假設好神經群之間的連結關係，才進行分析)，具有更廣泛的應用場景。</li>
<li>可同時分析不同型態的資料，包含離散型和連續型資料，也有助於分析跨尺度的交互關係(如神經元層級 vs 腦區層級)：
<ul>
<li>離散型資料：如有無產生動作電位、實驗動物有無產生特定行為模式</li>
<li>連續型資料：如膜電位變化、螢光強度變化、實驗動物位置或速度</li>
</ul>
</li>
<li>可偵測線性和非線性的交互關係</li>
<li>可用於多變量分析</li>
<li>一般而言，輸出的單位皆為bits (後文會加以介紹)，因此在不同實驗結果下比較會相對直觀(但並非可以直接比較)。
<br><br></li>
</ul>
<h2 id="資訊理論的限制">資訊理論的限制</h2>
<ul>
<li>無法建構描述系統如何運作的模型：<br>
如：分析結果得知，A與B共享了0.05 bits的資訊，但是我們無法更進一步知道A與B是否存在直接連結，即使有，也不知道是興奮性或抑制性連結。</li>
</ul>
<p>話雖如此，仍能透過資訊理論排除不可能的模型，限縮尋找目標的範圍。
<br><br></p>
<h2 id="資訊熵-shannon-entropy">資訊熵 (Shannon Entropy)</h2>
<p>首先，要得知訊息量多寡的方式，是這條訊息可以消除多少來自問題的不確定性(uncertainty)。</p>
<p>例如這個問題：今天晚餐要吃什麼？<br>
A：都可以<br>
B：校外好遠，在校內吃就好<br>
C：吃小7<br>
很明顯的，訊息量C &gt; B &gt; A，又A的回答不具任何訊息量。</p>
<p>因此，在測定訊息量之前，得先衡量出不確定性。Claude Shannon提出以資訊熵\(H(X)\) (Shannon entropy)來量化不確定性，不確定性越高，熵也就越大。<br>
(註：熵一詞源自於熱力學，用以描述系統無序的程度，資訊熵和熱力學的熵二者在定義及概念下皆具有相似性。)<br>
(註：為何是以\(H\)作為代號，可參考資料[4]。)</p>
<p>$$H(X) = - \sum_{x \in X} P(x)\ \log_{2}P(x)$$</p>
<p>公式中的\(X\)包含了所有可能狀態的\(x\)。
<br><br></p>
<ul>
<li>
<p>擲一公平硬幣，正面、反面機率皆為\(\frac{1}{2}\)，對於 &ldquo;朝上的是哪一面？&rdquo; 這個問題的不確定性：</p>
<p>$$H(X) = - \sum_{x \in {heads,tails}} P(x)\ \log_{2}P(x) = - [\frac{1}{2} \log_2(\frac{1}{2}) + \frac{1}{2} \log_2(\frac{1}{2})] = 1$$</p>
<p>擲一公平硬幣的熵(不確定性)為1 bit，我們可以理解為，用1個是非題能夠得到結果，也就是說提問 &ldquo;朝上的是正面嗎？&rdquo; ，若為是，則朝上的是正面，若為非，則朝上的是反面。
<br><br></p>
</li>
</ul>
<p>從上述例子得知，熵具有多少bits，就代表平均需要多少個是非題來求得最終狀態。
<br><br></p>
<ul>
<li>
<p>類似前一例，改擲一有問題的硬幣，正面機率\(\frac{4}{5}\)、反面機率\(\frac{1}{5}\)，直覺來說，這枚硬幣丟出後的不確定性會比公平硬幣來得小，因為我們知道他比較有可能是正面朝上。</p>
<p>$$H(X) = - \sum_{x \in {heads,tails}} P(x)\ \log_{2}P(x) = - [\frac{4}{5} \log_2(\frac{4}{5}) + \frac{1}{5} \log_2(\frac{1}{5})] \approx 0.72$$</p>
<p>的確，熵降到了0.72，也表現出當任一狀態出現的機率較高時，不確定性下降。
<br><br></p>
</li>
</ul>
<p>只有當各個狀態出現機率皆相同時，不確定性具有最大值(如Fig. 1 熵等於1 bit之處)；反之，若某一狀態出現機率為1，其他狀態就不可能發生，此時熵值為0，沒有不確定性(如Fig. 1 兩端)。</p>
<p><figure>
    <img src="/img/2020/1109/fig_1.png"
         alt="Fig. 1. 兩狀態機率分布對應之資訊熵值"/> <figcaption>
            <p>Fig. 1. 兩狀態機率分布對應之資訊熵值</p>
        </figcaption>
</figure>

<br></p>
<p>最後，資訊熵\(H(X)\)的公式具有幾項重要的性質：</p>
<ul>
<li>\(H(X) \ge 0\)，代表熵不為負值，負的不確定性沒有解釋意義。</li>
<li>當任一狀態絕對會發生(機率=1)時，熵值為零，沒有不確定性。</li>
<li>兩獨立變量的聯合熵(joint entropy)，會等於各自的熵值相加，表現出熵的可加成性。
(此文尚未提及聯合熵，會於下一篇中介紹。)
<br><br></li>
</ul>
<h2 id="小結">小結</h2>
<p>本文目前簡單介紹了資訊理論的優點和限制，並引入到資訊理論最基礎的量&ndash;熵，熵代表了不確定性，而資訊量則等同減少不確定性的程度。下一篇文中，將從一個變量增加到二個(可推廣至多變量)，並介紹聯合熵、條件熵(conditional entropy)、相互資訊(mutual information)等等在資訊理論中重要的概念。</p>
<p>

<style>
  #callout {
    background: #F9F9F9;
    padding: 1.5em 1.25em;
    border-radius: 3px;
    display: flex;
    flex-direction: row;
    margin-bottom: 20px;
  }
  #callout-inner {
    margin-left: 1em;
  }
  @media (max-width: 767px) {
    #callout {
    padding: 1.5em 0.75em 1.5em 0.6em;
    }
    #callout-inner {
      margin-left: 0.5em;
    }
  }
</style>

<div id="callout" style="">
  <div>💡</div>
	<div id="callout-inner">
    資訊代表了減少不確定性！
  </div>
</div>

<br></p>
<p>原始論文：</p>
<p>Timme, N. M. &amp; Lapish, C. A tutorial for information theory in neuroscience. <em>eNeuro</em> 5, (2018) doi:<a href='http://doi.org/10.1523/ENEURO.0052-18.2018' target='_blank'>10.1523/ENEURO.0052-18.2018</a>.
<br><br></p>
<p>參考文章：</p>
<ol>
<li>Cover, T. M. &amp; Thomas, J. A. Elements of information theory, 2nd Edition (Wiley-Interscience, 2006).</li>
<li><a href='http://ocw.nctu.edu.tw/course_detail.php?nid=612' target='_blank'>交通大學 陳伯寧老師 - 消息理論 Information Theory</a></li>
<li><a href='https://www.itsoc.org/resources' target='_blank'>Resources on Information Theory</a></li>
<li><a href='https://math.stackexchange.com/questions/84719/why-is-h-used-for-entropy' target='_blank'>Why is &ldquo;h&rdquo; used for entropy?</a></li>
</ol>

    </div>
    <footer>
      <div class="stats">
  
    <ul class="categories">
      
        
          <li><a class="article-terms-link" href="/categories/information-theory/">information theory</a></li>
        
      
    </ul>
  
  
    <ul class="tags">
      
        
          <li><a class="article-terms-link" href="/tags/information-theory/">information theory</a></li>
        
      
    </ul>
  
</div>

    </footer>
  </article>
  
    
  <article class="post">
    
    <div>
      <h2 id="say-something">Say Something</h2>
        <form id="comment-form" class="new-comment" method="POST">
          
          <h3 class='reply-notice hidden'>
            <span class='reply-name'></span>
          </h3>

          
          <input type="hidden" name="options[entryId]" value="a75758aa2f6820e842c0c0ed14b23a92">
          <input type='hidden' name='fields[replyThread]' value=''>
          <input type='hidden' name='fields[replyID]' value=''>
          <input type='hidden' name='fields[replyName]' value=''>

          
          <input required name='fields[name]' type='text' placeholder='Your Name'>
          <input name='fields[website]' type='text' placeholder='Your Website'>
          <input required name='fields[email]' type='email' placeholder='Your Email'>
          <textarea required name='fields[body]' placeholder='Your Message' rows='10'></textarea>

          
          

          
          <div class='submit-notice'>
            <strong class='submit-notice-text submit-success hidden'>Thanks for your comment! It will be shown on the site once it has been approved.</strong>
            <strong class='submit-notice-text submit-failed hidden'>Sorry, there was an error with your submission. Please make sure all required fields have been completed and try again.</strong>
          </div>

          
          <input type='submit' value='Submit' class='button'>
          <input type='submit' value='Submitted' class='hidden button' disabled>
          <input type='reset' value='Reset' class='button'>
        </form>
    </div>

    
    <div>
      <h2>Comments</h2><p>Nothing yet.</p>
      
    </div>
  </article>


  
  <div class="pagination">
    
      <a href="/blog/20210111-%E4%BA%BA%E9%A1%9E%E8%85%A6%E6%B3%A2%E8%88%87%E9%82%8A%E7%95%8C%E6%84%9F%E7%9F%A5/" class="button left"><span>人類腦波與邊界感知</span></a>
    
    
      <a href="/blog/20201012-%E8%A8%98%E6%86%B6%E5%BD%A2%E6%88%90%E7%9A%84%E5%88%86%E5%AD%90%E6%A9%9F%E5%88%B6/" class="button right"><span>記憶形成的分子機制</span></a>
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          <a href="/blog/20210111-%E4%BA%BA%E9%A1%9E%E8%85%A6%E6%B3%A2%E8%88%87%E9%82%8A%E7%95%8C%E6%84%9F%E7%9F%A5/" class="image" style="--bg-image: url('https://PikaPei.github.io/img/2021/0111/fig_1.png');">
    <img src="https://PikaPei.github.io/img/2021/0111/fig_1.png" alt="Proposed Model">
  </a>
        <header>
          <h2><a href="/blog/20210111-%E4%BA%BA%E9%A1%9E%E8%85%A6%E6%B3%A2%E8%88%87%E9%82%8A%E7%95%8C%E6%84%9F%E7%9F%A5/">人類腦波與邊界感知</a></h2>
          <time class="published" datetime="2021-01-11 00:00:00 &#43;0000 UTC">January 11, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/" class="image" style="--bg-image: url('https://PikaPei.github.io/img/2020/1109/entropy.jpg');">
    <img src="https://PikaPei.github.io/img/2020/1109/entropy.jpg" alt="Proposed Model">
  </a>
        <header>
          <h2><a href="/blog/20201109-%E7%A5%9E%E7%B6%93%E7%A7%91%E5%AD%B8%E8%88%87%E8%B3%87%E8%A8%8A%E7%90%86%E8%AB%96-part1/">神經科學與資訊理論 - Part 1</a></h2>
          <time class="published" datetime="2020-11-09 00:00:00 &#43;0000 UTC">November 9, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/20201012-%E8%A8%98%E6%86%B6%E5%BD%A2%E6%88%90%E7%9A%84%E5%88%86%E5%AD%90%E6%A9%9F%E5%88%B6/" class="image" style="--bg-image: url('https://PikaPei.github.io/img/2020/1012/fig_2.png');">
    <img src="https://PikaPei.github.io/img/2020/1012/fig_2.png" alt="Proposed Model">
  </a>
        <header>
          <h2><a href="/blog/20201012-%E8%A8%98%E6%86%B6%E5%BD%A2%E6%88%90%E7%9A%84%E5%88%86%E5%AD%90%E6%A9%9F%E5%88%B6/">記憶形成的分子機制</a></h2>
          <time class="published" datetime="2020-10-12 00:00:00 &#43;0000 UTC">October 12, 2020</time>
        </header>
      </article>
      
      
    </section>
  

  
    

      <section id="categories">
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/categories/information-theory/">information-theory<span class="count">1</span></a>
          
          <li>
              <a href="/categories/memory/">memory<span class="count">1</span></a>
          
          <li>
              <a href="/categories/navigation/">navigation<span class="count">1</span></a>
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>Explore The World of Neuroscience<br>Curious about Memory, Epilepsy, ASD ...</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        
        <li><a href="//github.com/PikaPei" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>









































      </ul>
  
  <p class="copyright">
    © 2021 PikaPei&#39;s Website
      <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.81.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="/js/highlight.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script><script src="/js/bundle.min.1ac7a98b06451b2770fdaaee2f67721ab7e137a5fee8352dc78a33e7a69b3244.js" integrity="sha256-GsepiwZFGydw/aruL2dyGrfhN6X&#43;6DUtx4oz56abMkQ="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-180152921-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>
